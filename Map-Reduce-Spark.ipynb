{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-JxbhK30e68",
        "outputId": "a2d01acd-3d20-44b1-8058-01cb116d90d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (91.189.91.83)] [Connecting to cloud.r-project.org] [Connectin\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,614 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,830 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,566 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,630 kB]\n",
            "Fetched 26.9 MB in 7s (3,709 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "OuA3dIaV0hV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip pagecounts-20160101-000000_parsed.out.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ4meA4Q0i-J",
        "outputId": "bb97ece6-c763-4854-b60f-44d64648c43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  pagecounts-20160101-000000_parsed.out.zip\n",
            "  inflating: pagecounts-20160101-000000_parsed.out  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq-n_uChF1Ii",
        "outputId": "c3ca262f-083c-430a-ff1e-0d4debef65b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WikimediaPageViewAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Replace \"your_data.out\" with the actual path/name of your extracted file\n",
        "rdd = spark.sparkContext \\\n",
        "    .textFile(\"/content/drive/MyDrive/pagecounts/pagecounts-20160101-000000_parsed.out\") \\\n",
        "    .filter(lambda line: line.strip() != \"\")  # Filter out any empty lines\n",
        "\n",
        "# Each line => [\"ace\", \"Asia\", \"1\", \"25859\"] etc.\n",
        "parsed_rdd = rdd.map(lambda line: line.split())\n",
        "\n",
        "# For clarity, create an RDD of tuples:\n",
        "# (project_code, page_title, page_hits, page_size)\n",
        "# converting page_hits and page_size to integers\n",
        "data_rdd = parsed_rdd.map(lambda x: (x[0], x[1], int(x[2]), int(x[3])) if len(x) == 4 else None) \\\n",
        "                      .filter(lambda x: x is not None)"
      ],
      "metadata": {
        "id": "Yadp7Lbv1R0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_mr_q1 = time.time()\n",
        "\n",
        "page_sizes = data_rdd.map(lambda row: row[3])  # row[3] is page_size\n",
        "\n",
        "min_size = page_sizes.reduce(lambda a, b: a if a < b else b)\n",
        "max_size = page_sizes.reduce(lambda a, b: a if a > b else b)\n",
        "\n",
        "# For average: sum all page sizes, then divide by count\n",
        "sum_sizes = page_sizes.reduce(lambda a, b: a + b)\n",
        "count_sizes = page_sizes.count()\n",
        "avg_size = sum_sizes / count_sizes if count_sizes > 0 else 0\n",
        "\n",
        "end_time_mr_q1 = time.time()\n",
        "\n",
        "print(\"=== Query 1 (MapReduce) ===\")\n",
        "print(f\"Min Page Size: {min_size}\")\n",
        "print(f\"Max Page Size: {max_size}\")\n",
        "print(f\"Average Page Size: {avg_size}\")\n",
        "print(f\"Time taken: {end_time_mr_q1 - start_time_mr_q1:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5v09Otq1SkU",
        "outputId": "5d7e3b93-78a5-4751-850d-c2d56db23208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 1 (MapReduce) ===\n",
            "Min Page Size: 0\n",
            "Max Page Size: 141180155987\n",
            "Average Page Size: 132215.79814237313\n",
            "Time taken: 66.5936 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_mr_q2 = time.time()\n",
        "\n",
        "titles_start_the = data_rdd.filter(lambda row: row[1].lower().startswith(\"the\"))\n",
        "\n",
        "total_the = titles_start_the.count()\n",
        "\n",
        "# Filter further for non-English\n",
        "non_en_the = titles_start_the.filter(lambda row: not row[0].startswith(\"en\")).count()\n",
        "\n",
        "end_time_mr_q2 = time.time()\n",
        "\n",
        "print(\"=== Query 2 (MapReduce) ===\")\n",
        "print(f\"Number of page titles starting with 'The': {total_the}\")\n",
        "print(f\"Number of those not in English project: {non_en_the}\")\n",
        "print(f\"Time taken: {end_time_mr_q2 - start_time_mr_q2:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc0Ah1v21WSB",
        "outputId": "7374bb90-695f-4a75-a47e-4ed4c256f4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 2 (MapReduce) ===\n",
            "Number of page titles starting with 'The': 45210\n",
            "Number of those not in English project: 9239\n",
            "Time taken: 35.0459 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_mr_q3 = time.time()\n",
        "\n",
        "# Split each title on '_', flatten, and get distinct\n",
        "unique_terms = data_rdd.flatMap(lambda row: row[1].lower().split(\"_\")).distinct().count()\n",
        "\n",
        "end_time_mr_q3 = time.time()\n",
        "\n",
        "print(\"=== Query 3 (MapReduce) ===\")\n",
        "print(f\"Number of unique terms in page titles: {unique_terms}\")\n",
        "print(f\"Time taken: {end_time_mr_q3 - start_time_mr_q3:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_QMth7F1X4r",
        "outputId": "a6b8351f-4aad-417c-963d-61035dd363b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 3 (MapReduce) ===\n",
            "Number of unique terms in page titles: 1793107\n",
            "Time taken: 41.4229 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_mr_q4 = time.time()\n",
        "\n",
        "# (title, 1) => reduceByKey to get counts\n",
        "title_counts = data_rdd.map(lambda row: (row[1], 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect to view or save results\n",
        "title_counts_result = title_counts.collect()\n",
        "\n",
        "end_time_mr_q4 = time.time()\n",
        "\n",
        "top_titles = sorted(title_counts_result, key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"=== Query 4 (MapReduce) ===\")\n",
        "print(\"Sample of title frequencies:\")\n",
        "for t, c in title_counts_result[:5]:  # just show first 5\n",
        "    print(f\"{t}: {c}\")\n",
        "print(f\"Time taken: {end_time_mr_q4 - start_time_mr_q4:.4f} seconds\\n\")\n",
        "print(\"=== Top 10 ===\")\n",
        "for t, c in top_titles:\n",
        "    print(f\"{t}: {c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSSN4OQV1ZZr",
        "outputId": "e76d3206-d577-4172-f093-c1754c11fede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 4 (MapReduce) ===\n",
            "Sample of title frequencies:\n",
            "E.Desv: 6\n",
            "Special:Contributions/5.232.61.79: 1\n",
            "Special:ListFiles/Nyttend: 1\n",
            "Special:WhatLinksHere/Main_Page: 8\n",
            "Time_Inc: 4\n",
            "Time taken: 42.1990 seconds\n",
            "\n",
            "=== Top 10 ===\n",
            "water: 118\n",
            "1863: 106\n",
            "Google: 101\n",
            "Berlin: 101\n",
            "Linux: 98\n",
            "Main_Page: 90\n",
            "ISO_3166-1: 88\n",
            "Microsoft_Windows: 87\n",
            "Index.php: 86\n",
            "HTML: 86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_mr_q5 = time.time()\n",
        "\n",
        "# groupByKey on title => RDD of (title, iterable_of_records)\n",
        "title_groups = data_rdd.map(lambda row: (row[1], row)) \\\n",
        "    .groupByKey()\n",
        "\n",
        "# Example to display how data is combined:\n",
        "combined_results = title_groups.mapValues(list).collect()\n",
        "\n",
        "end_time_mr_q5 = time.time()\n",
        "\n",
        "print(\"=== Query 5 (MapReduce) ===\")\n",
        "print(\"Sample combined data for the first few titles:\")\n",
        "for title, records in combined_results[:3]:  # just first 3\n",
        "    print(f\"Title: {title}\")\n",
        "    for r in records:\n",
        "        print(f\"    {r}\")\n",
        "print(f\"Time taken: {end_time_mr_q5 - start_time_mr_q5:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWxgqlgC1bA_",
        "outputId": "d66bc911-62d9-4cc8-8a3f-47fec254f801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 5 (MapReduce) ===\n",
            "Sample combined data for the first few titles:\n",
            "Title: E.Desv\n",
            "    ('aa', 'E.Desv', 1, 4662)\n",
            "    ('arc', 'E.Desv', 1, 5210)\n",
            "    ('ast', 'E.Desv', 1, 4825)\n",
            "    ('fiu-vro', 'E.Desv', 1, 5237)\n",
            "    ('fr', 'E.Desv', 1, 7057)\n",
            "    ('ik', 'E.Desv', 1, 4548)\n",
            "Title: Special:Contributions/5.232.61.79\n",
            "    ('aa', 'Special:Contributions/5.232.61.79', 1, 5805)\n",
            "Title: Special:ListFiles/Nyttend\n",
            "    ('aa', 'Special:ListFiles/Nyttend', 1, 5032)\n",
            "Time taken: 81.1058 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open output file for writing\n",
        "with open(\"Output.txt\", \"w\") as output_file:\n",
        "\n",
        "    # Query 1 (MapReduce)\n",
        "    output_file.write(\"=== Query 1 (MapReduce) ===\\n\")\n",
        "    output_file.write(f\"Min Page Size: {min_size}\\n\")\n",
        "    output_file.write(f\"Max Page Size: {max_size}\\n\")\n",
        "    output_file.write(f\"Average Page Size: {avg_size}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_mr_q1 - start_time_mr_q1:.4f} seconds\\n\\n\")\n",
        "\n",
        "    # Query 2 (MapReduce)\n",
        "    output_file.write(\"=== Query 2 (MapReduce) ===\\n\")\n",
        "    output_file.write(f\"Number of page titles starting with 'The': {total_the}\\n\")\n",
        "    output_file.write(f\"Number of those not in English project: {len(non_en_the)}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_mr_q2 - start_time_mr_q2:.4f} seconds\\n\\n\")\n",
        "\n",
        "    # Query 3 (MapReduce)\n",
        "    output_file.write(\"=== Query 3 (MapReduce) ===\\n\")\n",
        "    output_file.write(f\"Number of unique terms in page titles: {len(unique_terms)}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_mr_q3 - start_time_mr_q3:.4f} seconds\\n\\n\")\n",
        "\n",
        "    # Query 4 (MapReduce)\n",
        "    output_file.write(\"=== Query 4 (MapReduce) ===\\n\")\n",
        "    output_file.write(\"Sample of title frequencies:\\n\")\n",
        "    for t, c in title_counts_result[:5]:\n",
        "        output_file.write(f\"{t}: {c}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_mr_q4 - start_time_mr_q4:.4f} seconds\\n\")\n",
        "    output_file.write(\"=== Top 10 ===\\n\")\n",
        "    for t, c in top_titles:\n",
        "        output_file.write(f\"{t}: {c}\\n\")\n",
        "    output_file.write(\"\\n\")\n",
        "\n",
        "    # Query 5 (MapReduce)\n",
        "    output_file.write(\"=== Query 5 (MapReduce) ===\\n\")\n",
        "    output_file.write(\"Sample combined data for the first few titles:\\n\")\n",
        "    for title, records in combined_results[:3]:\n",
        "        output_file.write(f\"Title: {title}\\n\")\n",
        "        for r in records:\n",
        "            output_file.write(f\"    {r}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_mr_q5 - start_time_mr_q5:.4f} seconds\\n\\n\")"
      ],
      "metadata": {
        "id": "6WLanTzjMCON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_loops_q1 = time.time()\n",
        "\n",
        "# Collect all page sizes into a Python list\n",
        "page_sizes_list = data_rdd.map(lambda row: row[3]).collect()  # row[3] is page_size\n",
        "\n",
        "# Compute min, max, and average using Python functions\n",
        "min_size = min(page_sizes_list) if page_sizes_list else 0\n",
        "max_size = max(page_sizes_list) if page_sizes_list else 0\n",
        "avg_size = sum(page_sizes_list) / len(page_sizes_list) if page_sizes_list else 0\n",
        "\n",
        "end_time_loops_q1 = time.time()\n",
        "\n",
        "print(\"=== Query 1 (Spark Loops) ===\")\n",
        "print(f\"Min Page Size: {min_size}\")\n",
        "print(f\"Max Page Size: {max_size}\")\n",
        "print(f\"Average Page Size: {avg_size}\")\n",
        "print(f\"Time taken: {end_time_loops_q1 - start_time_loops_q1:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9PwwSqy87Zz",
        "outputId": "704d1eaf-59b2-435b-b794-9b2afb7a0a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 1 (Spark Loops) ===\n",
            "Min Page Size: 0\n",
            "Max Page Size: 141180155987\n",
            "Average Page Size: 132215.79814237313\n",
            "Time taken: 15.0592 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_loops_q2_2 = time.time()\n",
        "\n",
        "# Collect all rows into a Python list\n",
        "all_rows = data_rdd.collect()\n",
        "\n",
        "# Filter titles that start with \"The\" (case-insensitive)\n",
        "titles_start_the = [row for row in all_rows if row[1].lower().startswith(\"the\")]\n",
        "\n",
        "# Total number of such titles\n",
        "total_the = len(titles_start_the)\n",
        "\n",
        "# Filter further for titles not in English project (not starting with \"en\")\n",
        "non_en_the = [row for row in titles_start_the if not row[0].startswith(\"en\")]\n",
        "\n",
        "end_time_loops_q2 = time.time()\n",
        "\n",
        "print(\"=== Query 2 (Spark Loops) ===\")\n",
        "print(f\"Number of page titles starting with 'The': {total_the}\")\n",
        "print(f\"Number of those not in English project: {len(non_en_the)}\")\n",
        "print(f\"Time taken: {end_time_loops_q2 - start_time_loops_q2:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bim8dM-M87ri",
        "outputId": "10e7d669-39ec-401f-a08d-8802d283e5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 2 (Spark Loops) ===\n",
            "Number of page titles starting with 'The': 45210\n",
            "Number of those not in English project: 9239\n",
            "Time taken: 950.9160 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_loops_q3 = time.time()\n",
        "\n",
        "# Collect all titles into a Python list\n",
        "titles_list = data_rdd.map(lambda row: row[1].lower()).collect()\n",
        "\n",
        "# Split each title into terms (split by \"_\"), and collect unique terms using a set\n",
        "unique_terms = set()\n",
        "for title in titles_list:\n",
        "    terms = title.split(\"_\")\n",
        "    unique_terms.update(terms)  # Add terms to the set\n",
        "\n",
        "end_time_loops_q3 = time.time()\n",
        "\n",
        "print(\"=== Query 3 (Spark Loops) ===\")\n",
        "print(f\"Number of unique terms in page titles: {len(unique_terms)}\")\n",
        "print(f\"Time taken: {end_time_loops_q3 - start_time_loops_q3:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGB2eVPk871x",
        "outputId": "e5ccbcc2-8fd7-4778-fd9d-aa410f4aef49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 3 (Spark Loops) ===\n",
            "Number of unique terms in page titles: 1793107\n",
            "Time taken: 20.2332 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_loops_q4 = time.time()\n",
        "\n",
        "# Collect all titles into a Python list\n",
        "titles_list = data_rdd.map(lambda row: row[1]).collect()\n",
        "\n",
        "# Count frequency of each title using a dictionary\n",
        "title_counts = {}\n",
        "for title in titles_list:\n",
        "    if title not in title_counts:\n",
        "        title_counts[title] = 0\n",
        "    title_counts[title] += 1\n",
        "\n",
        "# Convert the dictionary to a sorted list of tuples (sorted by frequency)\n",
        "title_counts_result = list(title_counts.items())\n",
        "top_titles = sorted(title_counts_result, key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "end_time_loops_q4 = time.time()\n",
        "\n",
        "print(\"=== Query 4 (Spark Loops) ===\")\n",
        "print(\"Sample of title frequencies:\")\n",
        "for t, c in title_counts_result[:5]:  # just show first 5\n",
        "    print(f\"{t}: {c}\")\n",
        "print(f\"Time taken: {end_time_loops_q4 - start_time_loops_q4:.4f} seconds\\n\")\n",
        "print(\"=== Top 10 ===\")\n",
        "for t, c in top_titles:\n",
        "    print(f\"{t}: {c}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NISyLucR87_X",
        "outputId": "6e724328-04e3-4eaf-8ece-baec8084dcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 4 (Spark Loops) ===\n",
            "Sample of title frequencies:\n",
            "271_a.C: 4\n",
            "Category:User_th: 2\n",
            "Chiron_Elias_Krase: 6\n",
            "Dassault_rafaele: 3\n",
            "E.Desv: 6\n",
            "Time taken: 21.7222 seconds\n",
            "\n",
            "=== Top 10 ===\n",
            "water: 118\n",
            "1863: 106\n",
            "Berlin: 101\n",
            "Google: 101\n",
            "Linux: 98\n",
            "Main_Page: 90\n",
            "ISO_3166-1: 88\n",
            "Microsoft_Windows: 87\n",
            "Index.php: 86\n",
            "HTML: 86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Start timing --\n",
        "start_time_loops_q5 = time.time()\n",
        "\n",
        "# Collect all rows into a Python list\n",
        "all_rows = data_rdd.collect()\n",
        "\n",
        "# Group rows by page title using a dictionary\n",
        "title_groups = {}\n",
        "for row in all_rows:\n",
        "    title = row[1]  # row[1] is page_title\n",
        "    if title not in title_groups:\n",
        "        title_groups[title] = []\n",
        "    title_groups[title].append(row)\n",
        "\n",
        "# Convert the grouped data into a list of tuples for processing\n",
        "combined_results = list(title_groups.items())\n",
        "\n",
        "end_time_loops_q5 = time.time()\n",
        "\n",
        "print(\"=== Query 5 (Spark Loops) ===\")\n",
        "print(\"Sample combined data for the first few titles:\")\n",
        "for title, records in combined_results[:3]:  # just first 3\n",
        "    print(f\"Title: {title}\")\n",
        "    for r in records:\n",
        "        print(f\"    {r}\")\n",
        "print(f\"Time taken: {end_time_loops_q5 - start_time_loops_q5:.4f} seconds\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_eDk2O288Kj",
        "outputId": "47d749d5-6315-4ead-d280-6226d9a8876a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Query 5 (Spark Loops) ===\n",
            "Sample combined data for the first few titles:\n",
            "Title: 271_a.C\n",
            "    ('aa', '271_a.C', 1, 4675)\n",
            "    ('az', '271_a.C', 1, 6356)\n",
            "    ('bcl', '271_a.C', 1, 5068)\n",
            "    ('be', '271_a.C', 1, 6287)\n",
            "Title: Category:User_th\n",
            "    ('aa', 'Category:User_th', 1, 4770)\n",
            "    ('commons.m', 'Category:User_th', 1, 0)\n",
            "Title: Chiron_Elias_Krase\n",
            "    ('aa', 'Chiron_Elias_Krase', 1, 4694)\n",
            "    ('az', 'Chiron_Elias_Krase', 1, 6374)\n",
            "    ('bg', 'Chiron_Elias_Krase', 1, 7468)\n",
            "    ('cho', 'Chiron_Elias_Krase', 1, 4684)\n",
            "    ('dz', 'Chiron_Elias_Krase', 1, 5435)\n",
            "    ('it', 'Chiron_Elias_Krase', 1, 5929)\n",
            "Time taken: 38.0224 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open output file for writing\n",
        "with open(\"output_spark.txt\", \"w\") as output_file:\n",
        "\n",
        "    # Repeat the same process for Spark Loops queries:\n",
        "    # Query 1 (Spark Loops)\n",
        "    output_file.write(\"=== Query 1 (Spark Loops) ===\\n\")\n",
        "    output_file.write(f\"Min Page Size: {min_size}\\n\")\n",
        "    output_file.write(f\"Max Page Size: {max_size}\\n\")\n",
        "    output_file.write(f\"Average Page Size: {avg_size}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_loops_q1 - start_time_loops_q1:.4f} seconds\\n\\n\")\n",
        "\n",
        "    # Query 2 (Spark Loops)\n",
        "    output_file.write(\"=== Query 2 (Spark Loops) ===\\n\")\n",
        "    output_file.write(f\"Number of page titles starting with 'The': {total_the}\\n\")\n",
        "    output_file.write(f\"Number of those not in English project: {len(non_en_the)}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_loops_q2 - start_time_loops_q2:.4f} seconds\\n\\n\")\n",
        "\n",
        "    # Query 3 (Spark Loops)\n",
        "    output_file.write(\"=== Query 3 (Spark Loops) ===\\n\")\n",
        "    output_file.write(f\"Number of unique terms in page titles: {len(unique_terms)}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_loops_q3 - start_time_loops_q3:.4f} seconds\\n\\n\")\n",
        "\n",
        "    # Query 4 (Spark Loops)\n",
        "    output_file.write(\"=== Query 4 (Spark Loops) ===\\n\")\n",
        "    output_file.write(\"Sample of title frequencies:\\n\")\n",
        "    for t, c in title_counts_result[:5]:\n",
        "        output_file.write(f\"{t}: {c}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_loops_q4 - start_time_loops_q4:.4f} seconds\\n\")\n",
        "    output_file.write(\"=== Top 10 ===\\n\")\n",
        "    for t, c in top_titles:\n",
        "        output_file.write(f\"{t}: {c}\\n\")\n",
        "    output_file.write(\"\\n\")\n",
        "\n",
        "    # Query 5 (Spark Loops)\n",
        "    output_file.write(\"=== Query 5 (Spark Loops) ===\\n\")\n",
        "    output_file.write(\"Sample combined data for the first few titles:\\n\")\n",
        "    for title, records in combined_results[:3]:\n",
        "        output_file.write(f\"Title: {title}\\n\")\n",
        "        for r in records:\n",
        "            output_file.write(f\"    {r}\\n\")\n",
        "    output_file.write(f\"Time taken: {end_time_loops_q5 - start_time_loops_q5:.4f} seconds\\n\\n\")\n"
      ],
      "metadata": {
        "id": "LjVg9L3cIUo9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}